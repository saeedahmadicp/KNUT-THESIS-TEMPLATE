\newcommand{\thesisTitleEnglish}{Advancing Medical Image Segmentation Using Cross-Modal Knowledge Distillation and Hybrid CNN-Transformer}

{\centering \Large \bf \thesisTitleEnglish \par} \vspace{1.0cm}

Medical image segmentation is a critical and laborious task that involves precisely delineating anatomical structures, organs, and abnormalities within medical imaging data. This process plays a fundamental role in diagnosis, treatment planning, and quantitative analysis of various medical conditions. Manual analysis of these images requires the expertise of the radiologist to detect the abnormality and segment the imagery. However, this approach is susceptible to human error and is inherently limited in scalability. Consequently, there is a compelling need for automated analysis of medical images, which would expedite the process and extend its accessibility and efficiency to a broader population. This research focuses on contributing to the development of efficient automatic brain tumor segmentation algorithms. It proposes a configured and optimized hybrid residual attention UNet (COHRA-UNet) for multimodal brain tumor segmentation and a novel multi-teacher cross-modal knowledge distillation (MTCM-KD) framework for unimodal segmentation. 

The first study comprehensively examines multiple hyperparameters, including activation functions, normalization techniques, upsampling methods, and loss functions. We also conducted a detailed exploration of various attention mechanisms, encompassing spatial, channel, and transformer-based self-attention in diverse configurations, to evaluate their effects on the UNet architecture and identify their optimal settings for improving brain tumor segmentation performance. Additionally, we examined different hybridizations of the transformer-based CNN architecture. The proposed COHRA-UNet integrates these optimal parameters and configurations, demonstrating superior performance for the given task and surpassing previous state-of-the-art methods.

In our second study, we developed the MTCM-KD framework to tackle the issue of unavailable MRI modalities in clinical settings. This framework is specifically designed to segment the T\textsubscript{1ce}  MRI sequence, which is frequently used in clinical settings and shares structural similarities with the T\textsubscript{1} MRI scan and provides adequate information for effective segmentation. The framework integrates two distinct knowledge distillation (KD) strategies: 1) a performance-oriented response-based KD and 2) a cooperative deep supervision fusion learning (CDSFL). The former strategy adjusts confidence weights to each teacher model based on their performance, ensuring that the KD process is guided by performance. Moreover, the CDSFL module bolsters the learning capabilities of the multi-teacher models through facilitated mutual learning. Additionally, the combined knowledge from these models is distilled into the student model to enhance its deep learning supervision. Comprehensive testing on the BraTS datasets has shown that our framework delivers promising results in unimodal segmentation for both T\textsubscript{1ce} and T\textsubscript{1} modalities, surpassing earlier state-of-the-art techniques.